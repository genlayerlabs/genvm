bind_address: 127.0.0.1:3031

lua_script_path: ${genvmRoot}/scripts/genvm-llm-default-greyboxing.lua

threads: 4
blocking_threads: 16
log_disable: tracing*,polling*,tungstenite*,tokio_tungstenite*
#log_level: trace

backends:
  openai:
    host: https://api.openai.com
    provider: openai-compatible
    key: ${ENV[OPENAIKEY]}
    models:
      - gpt-4o-mini

  heurist:
    host: https://llm-gateway.heurist.xyz
    provider: openai-compatible
    key: ${ENV[HEURISTKEY]}
    models:
      - meta-llama/llama-3.3-70b-instruct

  anthropic:
    host: https://api.anthropic.com
    provider: anthropic
    key: ${ENV[ANTHROPICKEY]}
    models:
      - claude-3-7-sonnet-20250219

  xai:
    host: https://api.x.ai
    provider: openai-compatible
    key: ${ENV[XAIKEY]}
    models:
      - grok-2-1212

  google:
    host: https://generativelanguage.googleapis.com
    provider: google
    key: ${ENV[GEMINIKEY]}
    models:
      - gemini-1.5-flash

  atoma:
    enabled: false
    host: https://api.atoma.network
    provider: openai-compatible
    key: ${ENV[ATOMAKEY]}
    models:
      - meta-llama/Llama-3.3-70B-Instruct

prompt_templates:
  eq_comparative: |
    You are an a judge tasked with making a binary determination about whether two outputs satisfy the given comparison criteria.
    You are given two potential outputs to compare: the primary output and the comparison output.
    You must evaluate strictly based on the provided comparison criteria without introducing external criteria or assumptions.
    Your evaluation must be rigorous and thorough, as the stakes are high.

    Input sections:
    <primary_output>
    #{leader_answer}
    </primary_output>

    <comparison_output>
    #{validator_answer}
    </comparison_output>

    <comparison_criteria>
    #{principle}
    </comparison_criteria>

    Evaluation rules:

    1. If any section is missing or empty, return false
    2. Both outputs must satisfy ALL comparison criteria completely - partial satisfaction is insufficient
    3. If there is ANY ambiguity about whether a criterion is met, return false
    4. Evaluate using ONLY the explicitly stated comparison criteria
    5. Formatting differences alone do not affect equivalence unless specified in the criteria
    6. Do not make assumptions about unstated criteria or requirements

    Output format:
    Respond with json object containing key "result" and associated boolean value,
    representing the result of evaluating above criteria. And string field "reason",
    which contains extremely concise reason for decision.

    Examples:
    {"result": true, "reason": "all rules satisfied"}
    {"result": false, "reason": "rule 2 violated: ..."}

    Output must have only these two fields and not be wrapped in other objects.
  eq_non_comparative_leader: |
    You must perform given task for the input. Result must satisfy all of the criteria listed below
    You are tasked with performing a "task" for the given "input", your evaluation result (output) must be and satisfy all given criteria.
    You must evaluate strictly based on the provided comparison criteria and task without introducing external criteria or assumptions.
    Your result must be rigorous and thorough, as it's conformity will be evaluated in future.

    Input sections:
    <task>
    #{task}
    </task>

    <criteria>
    #{criteria}
    </criteria>

    <input>
    #{input}
    </input>

    Evaluation rules:

    1. Output must satisfy ALL comparison criteria completely - partial satisfaction is insufficient
    2. There must be NO ambiguity about whether a criterion is met
    3. Evaluate using ONLY the explicitly stated criteria and a task
    4. Do not make assumptions about unstated criteria or requirements

    Output format:
    Respond only with: the result of performing the task, do not include reasoning or evaluation rules satisfaction.
    If task requires outputting json, output valid parsable json without extra symbols.
  eq_non_comparative_validator: |
    You are an a judge tasked with making a binary determination about whether task evaluation result (output) is valid for the given input and satisfies given criteria.
    You must evaluate strictly based on the provided comparison criteria and task without introducing external criteria or assumptions.
    Your evaluation must be rigorous and thorough, as the stakes are high.

    Input sections:
    <task>
    #{task}
    </task>

    <input>
    #{input}
    </input>

    <criteria>
    #{criteria}
    </criteria>

    <output>
    #{output}
    </output>

    Evaluation rules:

    1. If any section is missing or empty, return false
    2. Output must satisfy ALL comparison criteria completely - partial satisfaction is insufficient
    3. If there is ANY ambiguity about whether a criterion is met, return false
    4. Evaluate using ONLY the explicitly stated criteria and a task
    5. Formatting differences alone do not affect the result unless specified in the criteria
    6. Do not make assumptions about unstated criteria or requirements

    Output format:
    Respond with json object containing key "result" and associated boolean value,
    representing the result of evaluating above criteria. And string field "reason",
    which contains extremely concise reason for decision.

    Examples:
    {"result": true, "reason": "all rules satisfied"}
    {"result": false, "reason": "rule 1 violated: section input is empty"}
    {"result": false, "reason": "criteria not met: <explanation>"}

    Output must have only these two fields and not be wrapped in other objects.
